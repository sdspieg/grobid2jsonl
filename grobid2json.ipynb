{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load doc2json venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the working directory in WSL format\n",
    "working_dir = r'/mnt/e/Google Drive/RuBase (1)/Corpora/RFSDP/Dissertations/ProQuest/240123'\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir(working_dir)\n",
    "\n",
    "# Verify that the working directory is set correctly\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Assuming 'working_dir' is your working directory path\n",
    "pickle_file_name = \"results.pickle\"  # The name of your pickle file\n",
    "\n",
    "# Construct the full path to the pickle file\n",
    "pickle_file_path = os.path.join(working_dir, pickle_file_name)\n",
    "\n",
    "# Read 'results' back from the pickle file\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    results_loaded = pickle.load(file)\n",
    "\n",
    "# Now, 'results_loaded' contains the data from the pickle file\n",
    "print(\"Loaded 'results' from the pickle file.\")\n",
    "\n",
    "# Takes about 3.6s (when in cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import bs4\n",
    "import re\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from typing import List, Dict, Tuple\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from doc2json.s2orc import Paper\n",
    "\n",
    "from doc2json.utils.grobid_util import parse_bib_entry, extract_paper_metadata_from_grobid_xml\n",
    "from doc2json.utils.citation_util import SINGLE_BRACKET_REGEX, BRACKET_REGEX, BRACKET_STYLE_THRESHOLD\n",
    "from doc2json.utils.citation_util import is_expansion_string, _clean_empty_and_duplicate_authors_from_grobid_parse\n",
    "from doc2json.utils.refspan_util import sub_spans_and_update_indices\n",
    "\n",
    "\n",
    "REPLACE_TABLE_TOKS = {\n",
    "    \"<row>\": \"<tr>\",\n",
    "    \"<row/>\": \"<tr/>\",\n",
    "    \"</row>\": \"</tr>\",\n",
    "    \"<cell>\": \"<td>\",\n",
    "    \"<cell/>\": \"<td/>\",\n",
    "    \"</cell>\": \"</td>\",\n",
    "    \"<cell \": \"<td \",\n",
    "    \"cols=\": \"colspan=\"\n",
    "}\n",
    "\n",
    "\n",
    "class UniqTokenGenerator:\n",
    "    \"\"\"\n",
    "    Generate unique token\n",
    "    \"\"\"\n",
    "    def __init__(self, tok_string):\n",
    "        self.tok_string = tok_string\n",
    "        self.ind = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        new_token = f'{self.tok_string}{self.ind}'\n",
    "        self.ind += 1\n",
    "        return new_token\n",
    "\n",
    "\n",
    "def normalize_grobid_id(grobid_id: str):\n",
    "    \"\"\"\n",
    "    Normalize grobid object identifiers\n",
    "    :param grobid_id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    str_norm = grobid_id.upper().replace('_', '').replace('#', '')\n",
    "    if str_norm.startswith('B'):\n",
    "        return str_norm.replace('B', 'BIBREF')\n",
    "    if str_norm.startswith('TAB'):\n",
    "        return str_norm.replace('TAB', 'TABREF')\n",
    "    if str_norm.startswith('FIG'):\n",
    "        return str_norm.replace('FIG', 'FIGREF')\n",
    "    if str_norm.startswith('FORMULA'):\n",
    "        return str_norm.replace('FORMULA', 'EQREF')\n",
    "    return str_norm\n",
    "\n",
    "\n",
    "def parse_bibliography(soup: BeautifulSoup) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Finds all bibliography entries in a grobid xml.\n",
    "    \"\"\"\n",
    "    bibliography = soup.listBibl\n",
    "    if bibliography is None:\n",
    "        return []\n",
    "\n",
    "    entries = bibliography.find_all(\"biblStruct\")\n",
    "\n",
    "    structured_entries = []\n",
    "    for entry in entries:\n",
    "        bib_entry = parse_bib_entry(entry)\n",
    "        # add bib entry only if it has a title\n",
    "        if bib_entry['title']:\n",
    "            structured_entries.append(bib_entry)\n",
    "\n",
    "    bibliography.decompose()\n",
    "\n",
    "    return structured_entries\n",
    "\n",
    "\n",
    "def extract_formulas_from_tei_xml(sp: BeautifulSoup) -> None:\n",
    "    \"\"\"\n",
    "    Replace all formulas with the text\n",
    "    :param sp:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for eq in sp.find_all('formula'):\n",
    "        eq.replace_with(sp.new_string(eq.text.strip()))\n",
    "\n",
    "\n",
    "def table_to_html(table: bs4.element.Tag) -> str:\n",
    "    \"\"\"\n",
    "    Sub table tags with html table tags\n",
    "    :param table_str:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for tag in table:\n",
    "        if tag.name != 'row':\n",
    "            print(f'Unknown table subtag: {tag.name}')\n",
    "            tag.decompose()\n",
    "    table_str = str(table)\n",
    "    for token, subtoken in REPLACE_TABLE_TOKS.items():\n",
    "        table_str = table_str.replace(token, subtoken)\n",
    "    return table_str\n",
    "\n",
    "\n",
    "def extract_figures_and_tables_from_tei_xml(sp: BeautifulSoup) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Generate figure and table dicts\n",
    "    :param sp:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ref_map = dict()\n",
    "\n",
    "    for fig in sp.find_all('figure'):\n",
    "        try:\n",
    "            if fig.name and fig.get('xml:id'):\n",
    "                if fig.get('type') == 'table':\n",
    "                    ref_map[normalize_grobid_id(fig.get('xml:id'))] = {\n",
    "                        \"text\": fig.figDesc.text.strip() if fig.figDesc else fig.head.text.strip() if fig.head else \"\",\n",
    "                        \"latex\": None,\n",
    "                        \"type\": \"table\",\n",
    "                        \"content\": table_to_html(fig.table),\n",
    "                        \"fig_num\": fig.get('xml:id')\n",
    "                    }\n",
    "                else:\n",
    "                    if True in [char.isdigit() for char in fig.findNext('head').findNext('label')]:\n",
    "                        fig_num = fig.findNext('head').findNext('label').contents[0]\n",
    "                    else:\n",
    "                        fig_num = None\n",
    "                    ref_map[normalize_grobid_id(fig.get('xml:id'))] = {\n",
    "                        \"text\": fig.figDesc.text.strip() if fig.figDesc else \"\",\n",
    "                        \"latex\": None,\n",
    "                        \"type\": \"figure\",\n",
    "                        \"content\": \"\",\n",
    "                        \"fig_num\": fig_num\n",
    "                    }\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        fig.decompose()\n",
    "\n",
    "    return ref_map\n",
    "\n",
    "\n",
    "def check_if_citations_are_bracket_style(sp: BeautifulSoup) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the document has bracket style citations\n",
    "    :param sp:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cite_strings = []\n",
    "    if sp.body:\n",
    "        for div in sp.body.find_all('div'):\n",
    "            if div.head:\n",
    "                continue\n",
    "            for rtag in div.find_all('ref'):\n",
    "                ref_type = rtag.get('type')\n",
    "                if ref_type == 'bibr':\n",
    "                    cite_strings.append(rtag.text.strip())\n",
    "\n",
    "        # check how many match bracket style\n",
    "        bracket_style = [bool(BRACKET_REGEX.match(cite_str)) for cite_str in cite_strings]\n",
    "\n",
    "        # return true if\n",
    "        if sum(bracket_style) > BRACKET_STYLE_THRESHOLD:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def sub_all_note_tags(sp: BeautifulSoup) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Sub all note tags with p tags\n",
    "    :param para_el:\n",
    "    :param sp:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for ntag in sp.find_all('note'):\n",
    "        p_tag = sp.new_tag('p')\n",
    "        p_tag.string = ntag.text.strip()\n",
    "        ntag.replace_with(p_tag)\n",
    "    return sp\n",
    "\n",
    "\n",
    "def process_formulas_in_paragraph(para_el: BeautifulSoup, sp: BeautifulSoup) -> None:\n",
    "    \"\"\"\n",
    "    Process all formulas in paragraph and replace with text and label\n",
    "    :param para_el:\n",
    "    :param sp:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for ftag in para_el.find_all('formula'):\n",
    "        # get label if exists and insert a space between formula and label\n",
    "        if ftag.label:\n",
    "            label = ' ' + ftag.label.text\n",
    "            ftag.label.decompose()\n",
    "        else:\n",
    "            label = ''\n",
    "        ftag.replace_with(sp.new_string(f'{ftag.text.strip()}{label}'))\n",
    "\n",
    "\n",
    "def process_references_in_paragraph(para_el: BeautifulSoup, sp: BeautifulSoup, refs: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Process all references in paragraph and generate a dict that contains (type, ref_id, surface_form)\n",
    "    :param para_el:\n",
    "    :param sp:\n",
    "    :param refs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tokgen = UniqTokenGenerator('REFTOKEN')\n",
    "    ref_dict = dict()\n",
    "    for rtag in para_el.find_all('ref'):\n",
    "        try:\n",
    "            ref_type = rtag.get('type')\n",
    "            # skip if citation\n",
    "            if ref_type == 'bibr':\n",
    "                continue\n",
    "            if ref_type == 'table' or ref_type == 'figure':\n",
    "                ref_id = rtag.get('target')\n",
    "                if ref_id and normalize_grobid_id(ref_id) in refs:\n",
    "                    # normalize reference string\n",
    "                    rtag_string = normalize_grobid_id(ref_id)\n",
    "                else:\n",
    "                    rtag_string = None\n",
    "                # add to ref set\n",
    "                ref_key = tokgen.next()\n",
    "                ref_dict[ref_key] = (rtag_string, rtag.text.strip(), ref_type)\n",
    "                rtag.replace_with(sp.new_string(f\" {ref_key} \"))\n",
    "            else:\n",
    "                # replace with surface form\n",
    "                rtag.replace_with(sp.new_string(rtag.text.strip()))\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    return ref_dict\n",
    "\n",
    "\n",
    "def process_citations_in_paragraph(para_el: BeautifulSoup, sp: BeautifulSoup, bibs: Dict, bracket: bool) -> Dict:\n",
    "    \"\"\"\n",
    "    Process all citations in paragraph and generate a dict for surface forms\n",
    "    :param para_el:\n",
    "    :param sp:\n",
    "    :param bibs:\n",
    "    :param bracket:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # CHECK if range between two surface forms is appropriate for bracket style expansion\n",
    "    def _get_surface_range(start_surface, end_surface):\n",
    "        span1_match = SINGLE_BRACKET_REGEX.match(start_surface)\n",
    "        span2_match = SINGLE_BRACKET_REGEX.match(end_surface)\n",
    "        if span1_match and span2_match:\n",
    "            # get numbers corresponding to citations\n",
    "            span1_num = int(span1_match.group(1))\n",
    "            span2_num = int(span2_match.group(1))\n",
    "            # expand if range is between 1 and 20\n",
    "            if 1 < span2_num - span1_num < 20:\n",
    "                return span1_num, span2_num\n",
    "        return None\n",
    "\n",
    "    # CREATE BIBREF range between two reference ids, e.g. BIBREF1-BIBREF4 -> BIBREF1 BIBREF2 BIBREF3 BIBREF4\n",
    "    def _create_ref_id_range(start_ref_id, end_ref_id):\n",
    "        start_ref_num = int(start_ref_id[6:])\n",
    "        end_ref_num = int(end_ref_id[6:])\n",
    "        return [f'BIBREF{curr_ref_num}' for curr_ref_num in range(start_ref_num, end_ref_num + 1)]\n",
    "\n",
    "    # CREATE surface form range between two bracket strings, e.g. [1]-[4] -> [1] [2] [3] [4]\n",
    "    def _create_surface_range(start_number, end_number):\n",
    "        return [f'[{n}]' for n in range(start_number, end_number + 1)]\n",
    "\n",
    "    # create citation dict with keywords\n",
    "    cite_map = dict()\n",
    "    tokgen = UniqTokenGenerator('CITETOKEN')\n",
    "\n",
    "    for rtag in para_el.find_all('ref'):\n",
    "        try:\n",
    "            # get surface span, e.g. [3]\n",
    "            surface_span = rtag.text.strip()\n",
    "\n",
    "            # check if target is available (#b2 -> BID2)\n",
    "            if rtag.get('target'):\n",
    "                # normalize reference string\n",
    "                rtag_ref_id = normalize_grobid_id(rtag.get('target'))\n",
    "\n",
    "                # skip if rtag ref_id not in bibliography\n",
    "                if rtag_ref_id not in bibs:\n",
    "                    cite_key = tokgen.next()\n",
    "                    rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                    cite_map[cite_key] = (None, surface_span)\n",
    "                    continue\n",
    "\n",
    "                # if bracket style, only keep if surface form is bracket\n",
    "                if bracket:\n",
    "                    # valid bracket span\n",
    "                    if surface_span and (surface_span[0] == '[' or surface_span[-1] == ']' or surface_span[-1] == ','):\n",
    "                        pass\n",
    "                    # invalid, replace tag with surface form and continue to next ref tag\n",
    "                    else:\n",
    "                        rtag.replace_with(sp.new_string(f\" {surface_span} \"))\n",
    "                        continue\n",
    "                # not bracket, add cite span and move on\n",
    "                else:\n",
    "                    cite_key = tokgen.next()\n",
    "                    rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                    cite_map[cite_key] = (rtag_ref_id, surface_span)\n",
    "                    continue\n",
    "\n",
    "                ### EXTRA PROCESSING FOR BRACKET STYLE CITATIONS; EXPAND RANGES ###\n",
    "                # look backward for range marker, e.g. [1]-*[3]*\n",
    "                backward_between_span = \"\"\n",
    "                for sib in rtag.previous_siblings:\n",
    "                    if sib.name == 'ref':\n",
    "                        break\n",
    "                    elif type(sib) == NavigableString:\n",
    "                        backward_between_span += sib\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                # check if there's a backwards expansion, e.g. need to expand [1]-[3] -> [1] [2] [3]\n",
    "                if is_expansion_string(backward_between_span):\n",
    "                    # get surface number range\n",
    "                    surface_num_range = _get_surface_range(\n",
    "                        rtag.find_previous_sibling('ref').text.strip(),\n",
    "                        surface_span\n",
    "                    )\n",
    "                    # if the surface number range is reasonable (range < 20, in order), EXPAND\n",
    "                    if surface_num_range:\n",
    "                        # delete previous ref tag and anything in between (i.e. delete \"-\" and extra spaces)\n",
    "                        for sib in rtag.previous_siblings:\n",
    "                            if sib.name == 'ref':\n",
    "                                break\n",
    "                            elif type(sib) == NavigableString:\n",
    "                                sib.replace_with(sp.new_string(\"\"))\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                        # get ref id of previous ref, e.g. [1] (#b0 -> BID0)\n",
    "                        previous_rtag = rtag.find_previous_sibling('ref')\n",
    "                        previous_rtag_ref_id = normalize_grobid_id(previous_rtag.get('target'))\n",
    "                        previous_rtag.decompose()\n",
    "\n",
    "                        # replace this ref tag with the full range expansion, e.g. [3] (#b2 -> BID1 BID2)\n",
    "                        id_range = _create_ref_id_range(previous_rtag_ref_id, rtag_ref_id)\n",
    "                        surface_range = _create_surface_range(surface_num_range[0], surface_num_range[1])\n",
    "                        replace_string = ''\n",
    "                        for range_ref_id, range_surface_form in zip(id_range, surface_range):\n",
    "                            # only replace if ref id is in bibliography, else add none\n",
    "                            if range_ref_id in bibs:\n",
    "                                cite_key = tokgen.next()\n",
    "                                cite_map[cite_key] = (range_ref_id, range_surface_form)\n",
    "                            else:\n",
    "                                cite_key = tokgen.next()\n",
    "                                cite_map[cite_key] = (None, range_surface_form)\n",
    "                            replace_string += cite_key + ' '\n",
    "                        rtag.replace_with(sp.new_string(f\" {replace_string} \"))\n",
    "                    # ELSE do not expand backwards and replace previous and current rtag with appropriate ref id\n",
    "                    else:\n",
    "                        # add mapping between ref id and surface form for previous ref tag\n",
    "                        previous_rtag = rtag.find_previous_sibling('ref')\n",
    "                        previous_rtag_ref_id = normalize_grobid_id(previous_rtag.get('target'))\n",
    "                        previous_rtag_surface = previous_rtag.text.strip()\n",
    "                        cite_key = tokgen.next()\n",
    "                        previous_rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                        cite_map[cite_key] = (previous_rtag_ref_id, previous_rtag_surface)\n",
    "\n",
    "                        # add mapping between ref id and surface form for current reftag\n",
    "                        cite_key = tokgen.next()\n",
    "                        rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                        cite_map[cite_key] = (rtag_ref_id, surface_span)\n",
    "                else:\n",
    "                    # look forward and see if expansion string, e.g. *[1]*-[3]\n",
    "                    forward_between_span = \"\"\n",
    "                    for sib in rtag.next_siblings:\n",
    "                        if sib.name == 'ref':\n",
    "                            break\n",
    "                        elif type(sib) == NavigableString:\n",
    "                            forward_between_span += sib\n",
    "                        else:\n",
    "                            break\n",
    "                    # look forward for range marker (if is a range, continue -- range will be expanded\n",
    "                    # when we get to the second value)\n",
    "                    if is_expansion_string(forward_between_span):\n",
    "                        continue\n",
    "                    # else treat like normal reference\n",
    "                    else:\n",
    "                        cite_key = tokgen.next()\n",
    "                        rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                        cite_map[cite_key] = (rtag_ref_id, surface_span)\n",
    "\n",
    "            else:\n",
    "                cite_key = tokgen.next()\n",
    "                rtag.replace_with(sp.new_string(f\" {cite_key} \"))\n",
    "                cite_map[cite_key] = (None, surface_span)\n",
    "        except AttributeError:\n",
    "            continue\n",
    "\n",
    "    return cite_map\n",
    "\n",
    "\n",
    "def process_paragraph(\n",
    "        sp: BeautifulSoup,\n",
    "        para_el: bs4.element.Tag,\n",
    "        section_names: List[Tuple],\n",
    "        bib_dict: Dict,\n",
    "        ref_dict: Dict,\n",
    "        bracket: bool\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Process one paragraph\n",
    "    :param sp:\n",
    "    :param para_el:\n",
    "    :param section_names:\n",
    "    :param bib_dict:\n",
    "    :param ref_dict:\n",
    "    :param bracket: if bracket style, expand and clean up citations\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # return empty paragraph if no text\n",
    "    if not para_el.text:\n",
    "        return {\n",
    "            'text': \"\",\n",
    "            'cite_spans': [],\n",
    "            'ref_spans': [],\n",
    "            'eq_spans': [],\n",
    "            'section': section_names\n",
    "        }\n",
    "\n",
    "    # replace formulas with formula text\n",
    "    process_formulas_in_paragraph(para_el, sp)\n",
    "\n",
    "    # get references to tables and figures\n",
    "    ref_map = process_references_in_paragraph(para_el, sp, ref_dict)\n",
    "\n",
    "    # generate citation map for paragraph element (keep only cite spans with bib entry or unlinked)\n",
    "    cite_map = process_citations_in_paragraph(para_el, sp, bib_dict, bracket)\n",
    "\n",
    "    # substitute space characters\n",
    "    para_text = re.sub(r'\\s+', ' ', para_el.text)\n",
    "    para_text = re.sub(r'\\s', ' ', para_text)\n",
    "\n",
    "    # get all cite and ref spans\n",
    "    all_spans_to_replace = []\n",
    "    for span in re.finditer(r'(CITETOKEN\\d+)', para_text):\n",
    "        uniq_token = span.group()\n",
    "        ref_id, surface_text = cite_map[uniq_token]\n",
    "        all_spans_to_replace.append((\n",
    "            span.start(),\n",
    "            span.start() + len(uniq_token),\n",
    "            uniq_token,\n",
    "            surface_text\n",
    "        ))\n",
    "    for span in re.finditer(r'(REFTOKEN\\d+)', para_text):\n",
    "        uniq_token = span.group()\n",
    "        ref_id, surface_text, ref_type = ref_map[uniq_token]\n",
    "        all_spans_to_replace.append((\n",
    "            span.start(),\n",
    "            span.start() + len(uniq_token),\n",
    "            uniq_token,\n",
    "            surface_text\n",
    "        ))\n",
    "\n",
    "    # replace cite and ref spans and create json blobs\n",
    "    para_text, all_spans_to_replace = sub_spans_and_update_indices(all_spans_to_replace, para_text)\n",
    "\n",
    "    cite_span_blobs = [{\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"text\": surface,\n",
    "        \"ref_id\": cite_map[token][0]\n",
    "    } for start, end, token, surface in all_spans_to_replace if token.startswith('CITETOKEN')]\n",
    "\n",
    "    ref_span_blobs = [{\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "        \"text\": surface,\n",
    "        \"ref_id\": ref_map[token][0]\n",
    "    } for start, end, token, surface in all_spans_to_replace if token.startswith('REFTOKEN')]\n",
    "\n",
    "    for cite_blob in cite_span_blobs:\n",
    "        assert para_text[cite_blob[\"start\"]:cite_blob[\"end\"]] == cite_blob[\"text\"]\n",
    "\n",
    "    for ref_blob in ref_span_blobs:\n",
    "        assert para_text[ref_blob[\"start\"]:ref_blob[\"end\"]] == ref_blob[\"text\"]\n",
    "\n",
    "    return {\n",
    "        'text': para_text,\n",
    "        'cite_spans': cite_span_blobs,\n",
    "        'ref_spans': ref_span_blobs,\n",
    "        'eq_spans': [],\n",
    "        'section': section_names\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_abstract_from_tei_xml(\n",
    "        sp: BeautifulSoup,\n",
    "        bib_dict: Dict,\n",
    "        ref_dict: Dict,\n",
    "        cleanup_bracket: bool\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse abstract from soup\n",
    "    :param sp:\n",
    "    :param bib_dict:\n",
    "    :param ref_dict:\n",
    "    :param cleanup_bracket:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    abstract_text = []\n",
    "    if sp.abstract:\n",
    "        # process all divs\n",
    "        if sp.abstract.div:\n",
    "            for div in sp.abstract.find_all('div'):\n",
    "                if div.text:\n",
    "                    if div.p:\n",
    "                        for para in div.find_all('p'):\n",
    "                            if para.text:\n",
    "                                abstract_text.append(\n",
    "                                    process_paragraph(sp, para, [(None, \"Abstract\")], bib_dict, ref_dict, cleanup_bracket)\n",
    "                                )\n",
    "                    else:\n",
    "                        if div.text:\n",
    "                            abstract_text.append(\n",
    "                                process_paragraph(sp, div, [(None, \"Abstract\")], bib_dict, ref_dict, cleanup_bracket)\n",
    "                            )\n",
    "        # process all paragraphs\n",
    "        elif sp.abstract.p:\n",
    "            for para in sp.abstract.find_all('p'):\n",
    "                if para.text:\n",
    "                    abstract_text.append(\n",
    "                        process_paragraph(sp, para, [(None, \"Abstract\")], bib_dict, ref_dict, cleanup_bracket)\n",
    "                    )\n",
    "        # else just try to get the text\n",
    "        else:\n",
    "            if sp.abstract.text:\n",
    "                abstract_text.append(\n",
    "                    process_paragraph(sp, sp.abstract, [(None, \"Abstract\")], bib_dict, ref_dict, cleanup_bracket)\n",
    "                )\n",
    "        sp.abstract.decompose()\n",
    "    return abstract_text\n",
    "\n",
    "\n",
    "def extract_body_text_from_div(\n",
    "        sp: BeautifulSoup,\n",
    "        div: bs4.element.Tag,\n",
    "        sections: List[Tuple],\n",
    "        bib_dict: Dict,\n",
    "        ref_dict: Dict,\n",
    "        cleanup_bracket: bool\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse body text from soup\n",
    "    :param sp:\n",
    "    :param div:\n",
    "    :param sections:\n",
    "    :param bib_dict:\n",
    "    :param ref_dict:\n",
    "    :param cleanup_bracket:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    # check if nested divs; recursively process\n",
    "    if div.div:\n",
    "        for subdiv in div.find_all('div'):\n",
    "            # has header, add to section list and process\n",
    "            if subdiv.head:\n",
    "                chunks += extract_body_text_from_div(\n",
    "                    sp,\n",
    "                    subdiv,\n",
    "                    sections + [(subdiv.head.get('n', None), subdiv.head.text.strip())],\n",
    "                    bib_dict,\n",
    "                    ref_dict,\n",
    "                    cleanup_bracket\n",
    "                )\n",
    "                subdiv.head.decompose()\n",
    "            # no header, process with same section list\n",
    "            else:\n",
    "                chunks += extract_body_text_from_div(\n",
    "                    sp,\n",
    "                    subdiv,\n",
    "                    sections,\n",
    "                    bib_dict,\n",
    "                    ref_dict,\n",
    "                    cleanup_bracket\n",
    "                )\n",
    "    # process tags individuals\n",
    "    for tag in div:\n",
    "        try:\n",
    "            if tag.name == 'p':\n",
    "                if tag.text:\n",
    "                    chunks.append(process_paragraph(\n",
    "                        sp, tag, sections, bib_dict, ref_dict, cleanup_bracket\n",
    "                    ))\n",
    "            elif tag.name == 'formula':\n",
    "                # e.g. <formula xml:id=\"formula_0\">Y = W T X.<label>(1)</label></formula>\n",
    "                label = tag.label.text\n",
    "                tag.label.decompose()\n",
    "                eq_text = tag.text\n",
    "                chunks.append({\n",
    "                    'text': 'EQUATION',\n",
    "                    'cite_spans': [],\n",
    "                    'ref_spans': [],\n",
    "                    'eq_spans': [\n",
    "                        {\n",
    "                            \"start\": 0,\n",
    "                            \"end\": 8,\n",
    "                            \"text\": \"EQUATION\",\n",
    "                            \"ref_id\": \"EQREF\",\n",
    "                            \"raw_str\": eq_text,\n",
    "                            \"eq_num\": label\n",
    "                        }\n",
    "                    ],\n",
    "                    'section': sections\n",
    "                })\n",
    "        except AttributeError:\n",
    "            if tag.text:\n",
    "                chunks.append(process_paragraph(\n",
    "                    sp, tag, sections, bib_dict, ref_dict, cleanup_bracket\n",
    "                ))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_body_text_from_tei_xml(\n",
    "        sp: BeautifulSoup,\n",
    "        bib_dict: Dict,\n",
    "        ref_dict: Dict,\n",
    "        cleanup_bracket: bool\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse body text from soup\n",
    "    :param sp:\n",
    "    :param bib_dict:\n",
    "    :param ref_dict:\n",
    "    :param cleanup_bracket:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    body_text = []\n",
    "    if sp.body:\n",
    "        body_text = extract_body_text_from_div(sp, sp.body, [], bib_dict, ref_dict, cleanup_bracket)\n",
    "        sp.body.decompose()\n",
    "    return body_text\n",
    "\n",
    "\n",
    "def extract_back_matter_from_tei_xml(\n",
    "        sp: BeautifulSoup,\n",
    "        bib_dict: Dict,\n",
    "        ref_dict: Dict,\n",
    "        cleanup_bracket: bool\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse back matter from soup\n",
    "    :param sp:\n",
    "    :param bib_dict:\n",
    "    :param ref_dict:\n",
    "    :param cleanup_bracket:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    back_text = []\n",
    "\n",
    "    if sp.back:\n",
    "        for div in sp.back.find_all('div'):\n",
    "            if div.get('type'):\n",
    "                section_type = div.get('type')\n",
    "            else:\n",
    "                section_type = ''\n",
    "\n",
    "            for child_div in div.find_all('div'):\n",
    "                if child_div.head:\n",
    "                    section_title = child_div.head.text.strip()\n",
    "                    section_num = child_div.head.get('n', None)\n",
    "                    child_div.head.decompose()\n",
    "                else:\n",
    "                    section_title = section_type\n",
    "                    section_num = None\n",
    "                if child_div.text:\n",
    "                    if child_div.text:\n",
    "                        back_text.append(\n",
    "                            process_paragraph(sp, child_div, [(section_num, section_title)], bib_dict, ref_dict, cleanup_bracket)\n",
    "                        )\n",
    "        sp.back.decompose()\n",
    "    return back_text\n",
    "\n",
    "\n",
    "def convert_tei_xml_soup_to_s2orc_json(soup: BeautifulSoup, paper_id: str, pdf_hash: str) -> Paper:\n",
    "    \"\"\"\n",
    "    Convert Grobid TEI XML to S2ORC json format\n",
    "    :param soup: BeautifulSoup of XML file content\n",
    "    :param paper_id: name of file\n",
    "    :param pdf_hash: hash of PDF\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # extract metadata\n",
    "    metadata = extract_paper_metadata_from_grobid_xml(soup.fileDesc)\n",
    "    # clean metadata authors (remove dupes etc)\n",
    "    metadata['authors'] = _clean_empty_and_duplicate_authors_from_grobid_parse(metadata['authors'])\n",
    "\n",
    "    # parse bibliography entries (removes empty bib entries)\n",
    "    biblio_entries = parse_bibliography(soup)\n",
    "    bibkey_map = {\n",
    "        normalize_grobid_id(bib['ref_id']): bib for bib in biblio_entries\n",
    "    }\n",
    "\n",
    "    # # process formulas and replace with text\n",
    "    # extract_formulas_from_tei_xml(soup)\n",
    "\n",
    "    # extract figure and table captions\n",
    "    refkey_map = extract_figures_and_tables_from_tei_xml(soup)\n",
    "\n",
    "    # get bracket style\n",
    "    is_bracket_style = check_if_citations_are_bracket_style(soup)\n",
    "\n",
    "    # substitute all note tags with p tags\n",
    "    soup = sub_all_note_tags(soup)\n",
    "\n",
    "    # process abstract if possible\n",
    "    abstract_entries = extract_abstract_from_tei_xml(soup, bibkey_map, refkey_map, is_bracket_style)\n",
    "\n",
    "    # process body text\n",
    "    body_entries = extract_body_text_from_tei_xml(soup, bibkey_map, refkey_map, is_bracket_style)\n",
    "\n",
    "    # parse back matter (acks, author statements, competing interests, abbrevs etc)\n",
    "    back_matter = extract_back_matter_from_tei_xml(soup, bibkey_map, refkey_map, is_bracket_style)\n",
    "\n",
    "    # form final paper entry\n",
    "    return Paper(\n",
    "        paper_id=paper_id,\n",
    "        pdf_hash=pdf_hash,\n",
    "        metadata=metadata,\n",
    "        abstract=abstract_entries,\n",
    "        body_text=body_entries,\n",
    "        back_matter=back_matter,\n",
    "        bib_entries=bibkey_map,\n",
    "        ref_entries=refkey_map\n",
    "    )\n",
    "\n",
    "def convert_tei_xml_string_to_s2orc_json(tei_xml_string: str, paper_id: str, pdf_hash: str = \"\") -> Paper:\n",
    "    \"\"\"\n",
    "    Convert a TEI XML string to S2ORC JSON format.\n",
    "    :param tei_xml_string: XML content as a string.\n",
    "    :param paper_id: Identifier for the paper.\n",
    "    :param pdf_hash: Hash of the corresponding PDF, if available.\n",
    "    :return: Paper object in S2ORC format.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(tei_xml_string, \"xml\")\n",
    "    return convert_tei_xml_soup_to_s2orc_json(soup, paper_id, pdf_hash)\n",
    "\n",
    "def process_paper(paper):\n",
    "    paper_data = {\n",
    "        'paper_id': str(paper.paper_id),\n",
    "        'pdf_hash': paper.pdf_hash,\n",
    "        'metadata': vars(paper.metadata) if paper.metadata else {},\n",
    "        'abstract': [vars(item) for item in paper.abstract] if paper.abstract else [],\n",
    "        'body_text': [vars(item) for item in paper.body_text] if paper.body_text else [],\n",
    "        'back_matter': [vars(item) for item in paper.back_matter] if paper.back_matter else [],\n",
    "        'bib_entries': [vars(item) for item in paper.bib_entries] if paper.bib_entries else [],\n",
    "        'ref_entries': [vars(item) for item in paper.ref_entries] if paper.ref_entries else [],\n",
    "        'raw_abstract_text': paper.raw_abstract_text,\n",
    "        'raw_body_text': paper.raw_body_text\n",
    "    }\n",
    "    return paper_data\n",
    "\n",
    "# Convert XML strings to Paper objects\n",
    "papers = []\n",
    "for paper_id, xml_content in tqdm(results_loaded.items(), desc=\"Processing XML\"):\n",
    "    paper = convert_tei_xml_string_to_s2orc_json(xml_content, paper_id)\n",
    "    papers.append(paper)\n",
    "\n",
    "# Convert the papers list to a list of dictionaries using the custom processing function\n",
    "papers_processed = []\n",
    "for paper in tqdm(papers, desc=\"Processing Papers\"):\n",
    "    processed_paper = process_paper(paper)\n",
    "    papers_processed.append(processed_paper)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_papers = pd.DataFrame(papers_processed)\n",
    "\n",
    "# Write the DataFrame to a JSONL file\n",
    "output_file_suffix = datetime.now().strftime(\"_%y%m%d\")\n",
    "output_jsonl_path = f'grobid2json{output_file_suffix}.jsonl'  # Adjust the path as needed\n",
    "df_papers.to_json(output_jsonl_path, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"Data written to JSONL file at: {output_jsonl_path}\")\n",
    "\n",
    "# Takes ~9 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Function to calculate text length\n",
    "def text_length(text):\n",
    "    return len(text) if isinstance(text, str) else 0\n",
    "\n",
    "# Calculate the length of text in each column\n",
    "df_papers['raw_abstract_text_length'] = df_papers['raw_abstract_text'].apply(text_length)\n",
    "df_papers['abstract_text_length'] = df_papers['abstract'].apply(lambda x: sum(text_length(item['text']) for item in x))\n",
    "df_papers['body_text_length'] = df_papers['body_text'].apply(lambda x: sum(text_length(item['text']) for item in x))\n",
    "df_papers['raw_body_text_length'] = df_papers['raw_body_text'].apply(text_length)\n",
    "\n",
    "# Create a subplot with 4 plots\n",
    "fig = make_subplots(rows=4, cols=1, subplot_titles=(\"Raw Abstract Text Length\", \"Abstract Text Length\", \"Body Text Length\", \"Raw Body Text Length\"))\n",
    "\n",
    "# Add histograms to the subplot with binsize of 10,000\n",
    "fig.add_trace(go.Histogram(x=df_papers['raw_abstract_text_length'], xbins=dict(start=0, end=max(df_papers['raw_abstract_text_length']), size=10000)), row=1, col=1)\n",
    "fig.add_trace(go.Histogram(x=df_papers['abstract_text_length'], xbins=dict(start=0, end=max(df_papers['abstract_text_length']), size=10000)), row=2, col=1)\n",
    "fig.add_trace(go.Histogram(x=df_papers['body_text_length'], xbins=dict(start=0, end=max(df_papers['body_text_length']), size=10000)), row=3, col=1)\n",
    "fig.add_trace(go.Histogram(x=df_papers['raw_body_text_length'], xbins=dict(start=0, end=max(df_papers['raw_body_text_length']), size=10000)), row=4, col=1)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(height=800, width=700, title_text=\"Distribution of Characters per 10,000 in Text Columns\", showlegend=False)\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load the pre-trained language detection model\n",
    "model = fasttext.load_model(\"lid.176.ftz\")\n",
    "\n",
    "# Function to predict language and extract the ISO code\n",
    "def detect_language(text):\n",
    "    # Replace newline characters with spaces\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # Predict the language\n",
    "    predictions = model.predict(text, k=1)\n",
    "    # Extract the language code\n",
    "    lang_code = predictions[0][0].replace(\"__label__\", \"\")\n",
    "    return lang_code\n",
    "\n",
    "# Detect language for each entry in 'raw_body_text' column\n",
    "df_papers['lang'] = [detect_language(text) for text in tqdm(df_papers['raw_body_text'], desc='Detecting Language')]\n",
    "\n",
    "# Now df_papers has a new column 'lang' with the language codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a new DataFrame with truncated 'raw_body_text' and 'lang'\n",
    "df_preview = pd.DataFrame({\n",
    "    'raw_body_text_truncated': df_papers['raw_body_text'].str[:2000],\n",
    "    'lang': df_papers['lang']\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_preview)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have your DataFrame df_papers already loaded\n",
    "\n",
    "# Use the value_counts() function to count unique values in the 'lang' column\n",
    "lang_counts = df_papers['lang'].value_counts().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "lang_counts.columns = ['Language', 'Count']\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "lang_counts = lang_counts.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(lang_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "# %pip install transformers\n",
    "# %pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install protobuf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming df_papers is your DataFrame\n",
    "print(\"Initializing the model and tokenizer...\")\n",
    "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name).to('cuda')\n",
    "\n",
    "def translate_mbart_batch(texts, src_lang_code, model, tokenizer):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Translating {len(texts)} texts from {src_lang_code} to English...\")\n",
    "    tokenizer.src_lang = src_lang_code\n",
    "    encoded = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to('cuda')\n",
    "    generated_tokens = model.generate(**encoded, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "    translations = [tokenizer.decode(tokens, skip_special_tokens=True) for tokens in generated_tokens]\n",
    "    print(f\"Completed translating {len(texts)} texts.\")\n",
    "    return translations\n",
    "\n",
    "# Prepare a new column for the translations\n",
    "df_papers['raw_body_text_en'] = None\n",
    "\n",
    "batch_size = 4  # Adjust based on your system's capabilities\n",
    "print(\"Starting the translation process...\")\n",
    "grouped = df_papers[df_papers['lang'] != 'en'].groupby('lang')\n",
    "\n",
    "with tqdm(total=len(df_papers), desc=\"Translating raw body texts\") as pbar:\n",
    "    for lang, group in grouped:\n",
    "        print(f\"Processing {len(group)} texts for language: {lang}...\")\n",
    "\n",
    "        texts = group['raw_body_text'].tolist()\n",
    "        texts = [text for text in texts if text]  # Remove empty texts\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            print(f\"Translating batch {i//batch_size + 1}: {len(batch_texts)} texts...\")\n",
    "            translated_texts = translate_mbart_batch(batch_texts, lang, model, tokenizer)\n",
    "            df_papers.loc[group.index[i:i+batch_size], 'raw_body_text_en'] = translated_texts\n",
    "            pbar.update(len(batch_texts))\n",
    "\n",
    "print(\"Translation process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_papers is your DataFrame\n",
    "\n",
    "# Filter the DataFrame for rows where 'lang' is 'en'\n",
    "en_rows = df_papers[df_papers['lang'] == 'en']\n",
    "\n",
    "# Select the first 10 rows from the filtered DataFrame\n",
    "first_10_en_rows = en_rows.head(10)\n",
    "\n",
    "# Print the first 2000 characters of 'raw_body_text' for these rows\n",
    "for index, row in first_10_en_rows.iterrows():\n",
    "    print(f\"Row {index}:\\n{row['raw_body_text'][:2000]}\\n{'-'*50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the examples DataFrame\n",
    "max_chars_body = 2000  # Adjust as needed\n",
    "examples = []\n",
    "\n",
    "for lang in df_papers['lang'].unique():\n",
    "    example_row = df_papers[df_papers['lang'] == lang].iloc[0]\n",
    "    truncated_body_original = example_row['raw_body_text'][:max_chars_body] if example_row['raw_body_text'] else \"\"\n",
    "    truncated_body_translated = example_row['raw_body_text_en'][:max_chars_body] if example_row['raw_body_text_en'] else \"\"\n",
    "    examples.append({'Language': lang,\n",
    "                     'Truncated Original Body Text': truncated_body_original,\n",
    "                     'Truncated Translated Body Text': truncated_body_translated})\n",
    "\n",
    "examples_df = pd.DataFrame(examples)\n",
    "print(examples_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_papers is your DataFrame and it's already loaded.\n",
    "\n",
    "# Only copy 'raw_body_text' to 'raw_body_text_en' for English language papers where 'raw_body_text' is not empty\n",
    "df_papers.loc[\n",
    "    (df_papers['lang'] == 'en') & (df_papers['raw_body_text'].notna()) & (df_papers['raw_body_text'] != ''), \n",
    "    'raw_body_text_en'\n",
    "] = df_papers.loc[\n",
    "    (df_papers['lang'] == 'en') & (df_papers['raw_body_text'].notna()) & (df_papers['raw_body_text'] != ''), \n",
    "    'raw_body_text'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_papers is your DataFrame and it's already loaded.\n",
    "\n",
    "# Filter to only English language papers\n",
    "english_papers = df_papers[df_papers['lang'] == 'en']\n",
    "\n",
    "# Take a sample of 10 random rows from the English papers\n",
    "sampled_english_papers = english_papers.sample(n=10)\n",
    "\n",
    "# Create the examples DataFrame with truncated text\n",
    "max_chars_body = 2000  # Adjust as needed\n",
    "examples = []\n",
    "\n",
    "for index, row in sampled_english_papers.iterrows():\n",
    "    truncated_body_original = row['raw_body_text'][:max_chars_body] if pd.notna(row['raw_body_text']) else \"\"\n",
    "    truncated_body_translated = row['raw_body_text_en'][:max_chars_body] if pd.notna(row['raw_body_text_en']) else \"\"\n",
    "    examples.append({\n",
    "        'Language': row['lang'],\n",
    "        'Truncated Original Body Text': truncated_body_original,\n",
    "        'Truncated Translated Body Text': truncated_body_translated\n",
    "    })\n",
    "\n",
    "examples_df = pd.DataFrame(examples)\n",
    "print(examples_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to count paragraphs in a text\n",
    "def count_paragraphs(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    paragraphs = text.split('\\n')\n",
    "    return len(paragraphs)\n",
    "\n",
    "# Apply the function to each row in the 'raw_body_text_en' column\n",
    "df_papers['paragraph_count'] = df_papers['raw_body_text_en'].apply(count_paragraphs)\n",
    "\n",
    "# Analyze the paragraph counts\n",
    "avg_paragraphs = df_papers['paragraph_count'].mean()\n",
    "min_paragraphs = df_papers['paragraph_count'].min()\n",
    "max_paragraphs = df_papers['paragraph_count'].max()\n",
    "\n",
    "print(f\"Average number of paragraphs per dissertation: {avg_paragraphs}\")\n",
    "print(f\"Minimum number of paragraphs in a dissertation: {min_paragraphs}\")\n",
    "print(f\"Maximum number of paragraphs in a dissertation: {max_paragraphs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assuming df_papers is your DataFrame and it already has the 'paragraph_count' column\n",
    "# Count dissertations with fewer than 50 paragraphs\n",
    "dissertations_fewer_than_50_paragraphs = (df_papers['paragraph_count'] < 50).sum()\n",
    "print(f\"Dissertations with fewer than 50 paragraphs: {dissertations_fewer_than_50_paragraphs}\")\n",
    "\n",
    "# Function to count sentences in a text\n",
    "def count_sentences(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    # Simple sentence split based on period followed by space or end of string\n",
    "    sentences = re.split(r'\\.\\s+|\\.$', text)\n",
    "    return len(sentences)\n",
    "\n",
    "# Apply the function to each row in the 'raw_body_text_en' column\n",
    "df_papers['sentence_count'] = df_papers['raw_body_text_en'].apply(count_sentences)\n",
    "\n",
    "# Analyze the sentence counts\n",
    "avg_sentences = df_papers['sentence_count'].mean()\n",
    "min_sentences = df_papers['sentence_count'].min()\n",
    "max_sentences = df_papers['sentence_count'].max()\n",
    "\n",
    "print(f\"Average number of sentences per dissertation: {avg_sentences}\")\n",
    "print(f\"Minimum number of sentences in a dissertation: {min_sentences}\")\n",
    "print(f\"Maximum number of sentences in a dissertation: {max_sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner', 'parser'])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "\n",
    "# Increase maximum text length (set to 2 million characters here, adjust as needed)\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Function to split text into sentences using spaCy\n",
    "def split_into_sentences(text):\n",
    "    if pd.isna(text) or len(text) == 0:\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Assuming df_papers is your DataFrame and it has a column named 'raw_body_text_en'\n",
    "# Create a new DataFrame for sentences\n",
    "df_papers_sents = pd.DataFrame(columns=['paper_id', 'sentence'])\n",
    "\n",
    "# Process each row in df_papers\n",
    "for index, row in tqdm(df_papers.iterrows(), total=len(df_papers), desc=\"Processing dissertations\"):\n",
    "    sentences = split_into_sentences(row['raw_body_text_en'])\n",
    "    temp_df = pd.DataFrame({'paper_id': index, 'sentence': sentences})\n",
    "    df_papers_sents = pd.concat([df_papers_sents, temp_df], ignore_index=True)\n",
    "\n",
    "# Write out the results to a JSONL file\n",
    "output_filename = f'df_papers_sents_{datetime.now().strftime(\"%y%m%d\")}.jsonl'\n",
    "df_papers_sents.to_json(output_filename, orient='records', lines=True)\n",
    "\n",
    "print(f\"Data written to {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install 'urllib3<2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U --pre \"weaviate-client==v4.4b2\"\n",
    "# %pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers_sents.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import weaviate\n",
    "import json\n",
    "\n",
    "# Initialize Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Function to Compute Embeddings\n",
    "def compute_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Return a list of floats for the embedding\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
    "\n",
    "# Initialize Weaviate Client\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Validation Function\n",
    "def is_valid_data(data_object):\n",
    "    # Add validation logic here\n",
    "    return True  # Return True if valid, False otherwise\n",
    "\n",
    "# Load and Slice DataFrame for First 15 Rows\n",
    "df_papers_sents = df_papers_sents.head(15)\n",
    "\n",
    "# Check for Empty 'sentence' Column\n",
    "empty_sentences = df_papers_sents[df_papers_sents['sentence'].isna() | (df_papers_sents['sentence'] == '')]\n",
    "if not empty_sentences.empty:\n",
    "    print(f\"Empty sentences found in rows: {empty_sentences.index.tolist()}\")\n",
    "else:\n",
    "    print(\"No empty sentences found.\")\n",
    "\n",
    "# Process Data and Store in Weaviate\n",
    "for index, row in tqdm(df_papers_sents.iterrows(), total=df_papers_sents.shape[0], desc=\"Processing Rows\"):\n",
    "    text = row['sentence']\n",
    "    if pd.notna(text) and text.strip():  # Check if the text is not empty or NaN\n",
    "        embedding = compute_embeddings(text)\n",
    "        \n",
    "        # Debugging: Print the type and a portion of the embedding\n",
    "        print(f\"Embedding type: {type(embedding)}\")\n",
    "        print(f\"Embedding sample: {embedding[:10]}\")\n",
    "\n",
    "        data_object = {\n",
    "            \"embedding\": embedding,\n",
    "            \"text\": text,\n",
    "            \"reference\": index\n",
    "        }\n",
    "        unique_id = row['uuid']\n",
    "        try:\n",
    "            client.data_object.create(data_object, \"RFSDPDissesEmbeddingEn\", unique_id)\n",
    "            print(f\"Added document embedding with UUID: {unique_id}\")\n",
    "        except weaviate.UnexpectedStatusCodeException as e:\n",
    "            error_details = str(e)\n",
    "            print(f\"Error adding document embedding at index {index}: {error_details}\")\n",
    "\n",
    "print(\"Completed processing and storing embeddings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm\n",
    "import weaviate\n",
    "\n",
    "# Initialize Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model = AutoModel.from_pretrained(\"BAAI/bge-large-en-v1.5\")\n",
    "model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Function to Compute Embeddings\n",
    "def compute_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy().tolist()\n",
    "\n",
    "# Initialize Weaviate Client\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "# Delete existing class (if needed)\n",
    "try:\n",
    "    client.schema.delete_class(\"RFSDPDissesEmbeddingEn\")\n",
    "    print(\"Class RFSDPDissesEmbeddingEn deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting class: {e}\")\n",
    "\n",
    "# Create new class schema\n",
    "class_schema = {\n",
    "    \"class\": \"RFSDPDissesEmbeddingEn\",\n",
    "    \"description\": \"Store embeddings of sentences from dissertations\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"The text of the sentence\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"embedding\",\n",
    "            \"dataType\": [\"number\"],\n",
    "            \"description\": \"The embedding vector\",\n",
    "            \"vectorIndexType\": \"hnsw\",\n",
    "            \"vectorizer\": \"none\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"reference\",\n",
    "            \"dataType\": [\"int\"],\n",
    "            \"description\": \"A reference to the original paper\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create new class\n",
    "try:\n",
    "    client.schema.create_class(class_schema)\n",
    "    print(\"Class RFSDPDissesEmbeddingEn created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating class: {e}\")\n",
    "\n",
    "# Load DataFrame (Assuming it's already available as df_papers_sents)\n",
    "# df_papers_sents = pd.read_csv('your_csv_file.csv') # Uncomment if needed\n",
    "\n",
    "# Add a UUID column to the DataFrame\n",
    "df_papers_sents['uuid'] = [str(uuid.uuid4()) for _ in range(len(df_papers_sents))]\n",
    "\n",
    "# Process Data and Store in Weaviate\n",
    "batch_size = 256  # Adjust as needed\n",
    "for start_idx in tqdm(range(0, len(df_papers_sents), batch_size), desc=\"Processing Batches\"):\n",
    "    end_idx = min(start_idx + batch_size, len(df_papers_sents))\n",
    "    batch_df = df_papers_sents.iloc[start_idx:end_idx]\n",
    "\n",
    "    for index, row in batch_df.iterrows():\n",
    "        text = row['sentence']\n",
    "        if pd.notna(text) and text.strip():  # Check if the text is not empty or NaN\n",
    "            embedding = compute_embeddings(text)\n",
    "            data_object = {\n",
    "                \"embedding\": embedding,\n",
    "                \"text\": text,\n",
    "                \"reference\": index\n",
    "            }\n",
    "            unique_id = row['uuid']\n",
    "            try:\n",
    "                client.data_object.create(data_object, \"RFSDPDissesEmbeddingEn\", unique_id)\n",
    "                print(f\"Added document embedding with UUID: {unique_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding document embedding: {e}\")\n",
    "\n",
    "print(\"Completed processing and storing embeddings.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc2json",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
